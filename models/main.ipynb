{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANI-1 Molecular Potential Prediction using Pre-trained EGNN and Transformer-Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file is used to record the complete process and workflow of model training/fine-tuning. Data processing classes, utility functions and model substructures were encapsulated in the models folder, which will not be shown in this file. If needed, please refer to the [Github Repository](https://github.com/Curtis-Wu/Equivariant-Graph-Transformer)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General Import Statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import csv\n",
    "import time\n",
    "import yaml\n",
    "import h5py\n",
    "import math\n",
    "import shutil\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "from datetime import datetime\n",
    "from utils import adjust_learning_rate\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.multiprocessing\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW\n",
    "from torch_scatter import scatter\n",
    "from torch_cluster import radius_graph\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "torch.multiprocessing.set_sharing_strategy('file_system')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Architecture Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "  # EGNN part of the model\n",
    "  \"hidden_channels\": 256,    # Number of hidden_channels\n",
    "  \"num_edge_feats\": 0,           # Number of additional edge features\n",
    "  \"num_egcl\": 5,             # Number of EGCL layers\n",
    "  \"residual\": True,          # Residual calculation\n",
    "  \"attention\": True,         # Graph Attention mechanism\n",
    "  \"normalize\": True,         # Interatomic distance normalization\n",
    "  \"tanh\": False,             # Additional activation after each layer\n",
    "  \"cutoff\": 5.0,            # Interatomic distance curoff\n",
    "  \"max_atom_type\": 28,       # Max atom types\n",
    "  \"max_chirality_type\": 5,   # Max chirality type\n",
    "  \"max_num_neighbors\": 32,   # Max number of neighborgoods\n",
    "\n",
    "  # Transformer-Encoder part of the model\n",
    "  \"d_model\": 256,            # Embeddings for each token\n",
    "  \"num_heads\": 4,            # Number of self-attention heads\n",
    "  \"dropout_r\": 0.1,          # Dropout rate\n",
    "  \"num_ffn\": 256,            # Number of neurons in the feedforward MLP\n",
    "  \"num_encoder\": 2,          # Number of encoder units\n",
    "\n",
    "  # Energy Head\n",
    "  \"num_neuron\": 512         # NUmber of neurons for the final energy head\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from EGNN import E_GCL\n",
    "from TF_Encoder import EncoderLayer\n",
    "\n",
    "class EGTF(nn.Module):\n",
    "    def __init__(self, # EGNN/EGCL parameters\n",
    "                 hidden_channels, num_edge_feats = 0, num_egcl = 4, \n",
    "                 act_fn = nn.SiLU(), residual = True, attention = True,\n",
    "                 normalize = False, tanh = False, max_atom_type = 100, \n",
    "                 cutoff = 5.0, max_num_neighbors = 32, \n",
    "                 # Transformer-Encoder parameters\n",
    "                 d_model = 256, num_encoder = 2, num_heads = 8,\n",
    "                 num_ffn = 512, dropout_r = 0.1,\n",
    "                 # Energy Head parameter\n",
    "                 num_neurons = 512):\n",
    "\n",
    "        super(EGTF, self).__init__()\n",
    "        # self.hidden_channels = hidden_channels\n",
    "        self.n_layers = num_egcl\n",
    "        # self.max_atom_type = max_atom_type\n",
    "        self.cutoff = cutoff\n",
    "        self.max_num_neighbors = max_num_neighbors\n",
    "        # Create embeddings of dimension (hidden_channels, ) for each atom type\n",
    "        self.type_embedding = nn.Embedding(max_atom_type, hidden_channels)\n",
    "        \n",
    "        # EGC layers\n",
    "        for i in range(0, num_egcl):\n",
    "            self.add_module(\"gcl_%d\" % i, E_GCL(\n",
    "                input_nf = hidden_channels, \n",
    "                output_nf = hidden_channels, \n",
    "                hidden_nf = hidden_channels, \n",
    "                add_edge_feats = num_edge_feats,\n",
    "                act_fn=act_fn, residual=residual, \n",
    "                attention=attention, normalize=normalize, tanh=tanh))\n",
    "\n",
    "        # Transformer-Encoder layers\n",
    "        self.encoder_layers = nn.ModuleList([EncoderLayer(d_model, num_heads, num_ffn, dropout_r) \n",
    "                                             for _ in range(num_encoder)])\n",
    "\n",
    "        # Energy Head\n",
    "        self.energy_fc = nn.Sequential(\n",
    "            nn.Linear(num_neurons, num_neurons),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(num_neurons, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, z, pos, batch, edge_index=None, edge_feats=None):\n",
    "        h = self.type_embedding(z)\n",
    "        x = deepcopy(pos)\n",
    "        if edge_index is None:\n",
    "            # Calculates edge_index from graph structure based on cutoff radius\n",
    "            edge_index = radius_graph(\n",
    "                pos,\n",
    "                r=self.cutoff,\n",
    "                batch=batch,\n",
    "                loop=False,\n",
    "                max_num_neighbors=self.max_num_neighbors + 1,\n",
    "            )\n",
    "        # EGC layers\n",
    "        for i in range(0, self.n_layers):\n",
    "            h, x, _ = self._modules[\"gcl_%d\" % i](h, edge_index, x, edge_feats=edge_feats)\n",
    "        # Encoder layers\n",
    "        for layer in self.encoder_layers:\n",
    "            x = layer(x)\n",
    "        # Energy Head\n",
    "        out = self.energy_fc(x)\n",
    "\n",
    "        return out        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained model\n",
    "pretrained_model = torch.load('model.pth', map_location=torch.device('cpu'))\n",
    "# Initialize the modified model\n",
    "modified_model = EGTF(# EGNN/EGCL parameters\n",
    "                 hidden_channels = 256, num_edge_feats = 0, num_egcl = 3, \n",
    "                 act_fn = nn.SiLU(), residual = True, attention = True,\n",
    "                 normalize = False, tanh = False, max_atom_type = 28, \n",
    "                 cutoff = 5.0, max_num_neighbors = 32, \n",
    "                 # Transformer-Encoder parameters\n",
    "                 d_model = 256, num_encoder = 2, num_heads = 8,\n",
    "                 num_ffn = 512, dropout_r = 0.1,\n",
    "                 # Energy Head parameter\n",
    "                 num_neurons = 256)\n",
    "# Load weights\n",
    "modified_model.load_state_dict(pretrained_model, strict=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Processing and Model Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_potential",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
