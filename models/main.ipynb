{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANI-1 Molecular Potential Prediction using Pre-trained EGNN and Transformer-Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file is used to record the complete process and workflow of data-processing, model architecture creation, model training/fine-tuning. Some utility functions were encapsulated in the models folder in the [[GitHub Repository]](https://github.com/Curtis-Wu/Equivariant-Graph-Transformer), which will not be shown in this file.\n",
    "\n",
    "The goal of this project is to achieve accurate molecular potential prediction for the ANI-1 data set. But the functionality of the model should be able to generalize molecular properties prediction accurately to other datasets such as ANI-1x, QM9 etc. This was achieved by using end-to-end training/prediction, ie. not using hand-crafted molecular descriptors/features such as Atomic Environment Vectors (AEV) as described in the [[2017 Paper]](https://pubs.rsc.org/en/content/articlelanding/2017/sc/c6sc05720a).\n",
    "\n",
    "The model presented in this repository use a [(Pre-trained)](https://pubs.acs.org/doi/10.1021/acs.jctc.3c00289) [E(n) equivariant neural network](https://arxiv.org/abs/2102.09844), which becomes invariant in our case when dealing with objects with static positions, as well as an transformer encoder to capture both the local and global interactions between the point clouds to achieve molecular properties predictions accurately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <b>Model Architecture</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <i>Helper Functions and Preliminary Imports</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arimelf/miniconda3/envs/ml_potential/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import csv\n",
    "import time\n",
    "import yaml\n",
    "import math\n",
    "import shutil\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.multiprocessing\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW\n",
    "from torch_scatter import scatter\n",
    "from torch_cluster import radius_graph\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "torch.multiprocessing.set_sharing_strategy('file_system')\n",
    "\n",
    "\n",
    "def unsorted_segment_sum(data, segment_ids, num_segments):\n",
    "    result_shape = (num_segments, data.size(1))\n",
    "    result = data.new_full(result_shape, 0)  # Init empty result tensor.\n",
    "    segment_ids = segment_ids.unsqueeze(-1).expand(-1, data.size(1))\n",
    "    result.scatter_add_(0, segment_ids, data)\n",
    "    return result\n",
    "\n",
    "def unsorted_segment_mean(data, segment_ids, num_segments):\n",
    "    result_shape = (num_segments, data.size(1))\n",
    "    segment_ids = segment_ids.unsqueeze(-1).expand(-1, data.size(1))\n",
    "    result = data.new_full(result_shape, 0)  # Init empty result tensor.\n",
    "    count = data.new_full(result_shape, 0)\n",
    "    result.scatter_add_(0, segment_ids, data)\n",
    "    count.scatter_add_(0, segment_ids, torch.ones_like(data))\n",
    "    return result / count.clamp(min=1)\n",
    "\n",
    "def single_head_attention(q, k, v):\n",
    "    # attention(q, k, v) = softmax(qK.T/sqrt(dk)V)\n",
    "    d_k = q.size()[-1] # 64\n",
    "    # Only transpose the last 2 dimensions, because the first dimension is the batch size\n",
    "    # scale the value with square root of d_k which is a constant value\n",
    "    val_before_softmax = torch.matmul(q, k.transpose(-1,-2))/math.sqrt(d_k)\n",
    "    attention = F.softmax(val_before_softmax, dim = -1) # 200 x 200\n",
    "    # Multiply attention matrix with value matrix\n",
    "    values = torch.matmul(attention, v) # 200 x 64\n",
    "    return values, attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <i>Equivariant Graph Convolutional Layer (EGCL)</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class E_GCL(nn.Module):\n",
    "    \"\"\"\n",
    "    E(n) Equivariant Convolutional Layer\n",
    "    Reference: EGNN: V. G. Satorras et al., https://arxiv.org/abs/2102.09844 \n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, input_nf, output_nf, hidden_nf, add_edge_feats=0, act_fn=nn.SiLU(), \n",
    "        residual=True, attention=False, normalize=False, coords_agg='mean', static_coord = True\n",
    "    ):\n",
    "        '''\n",
    "        :param intput_nf: Number of input node features\n",
    "        :param output_nf: Number of output node features\n",
    "        :param hidden_nf: Number of hidden node features\n",
    "        :param add_edge_feats: Number of additional edge feature\n",
    "        :param act_fn: Activation function\n",
    "        :param residual: Use residual connections\n",
    "        :param attention: Whether using attention or not\n",
    "        :param normalize: Normalizes the coordinates messages such that:\n",
    "                    instead of: x^{l+1}_i = x^{l}_i + Σ(x_i - x_j)phi_x(m_ij)\n",
    "                    we get:     x^{l+1}_i = x^{l}_i + Σ(x_i - x_j)phi_x(m_ij)/||x_i - x_j||\n",
    "        :param coords_agg: aggregation function\n",
    "        '''\n",
    "        super(E_GCL, self).__init__()\n",
    "        self.residual = residual\n",
    "        self.attention = attention\n",
    "        self.normalize = normalize\n",
    "        self.static_coord = static_coord\n",
    "        self.coords_agg = coords_agg\n",
    "        # Number of features used to describe the relative positions between nodes\n",
    "        # Because we're using radial distance, so dimension = 1\n",
    "        edge_coords_nf = 1\n",
    "        # input_edge stores the node values, one edge connects two nodes, so * 2\n",
    "        input_edge = input_nf * 2\n",
    "        # Prevents division by zeroes, numerical stability purpose\n",
    "        self.epsilon = 1e-8\n",
    "        # mlp operation for edges\n",
    "        self.edge_mlp = nn.Sequential(\n",
    "            nn.Linear(input_edge + edge_coords_nf + add_edge_feats, hidden_nf),\n",
    "            act_fn,\n",
    "            nn.Linear(hidden_nf, hidden_nf),\n",
    "            act_fn)\n",
    "        # mlp operation for nodes\n",
    "        self.node_mlp = nn.Sequential(\n",
    "            nn.Linear(hidden_nf + input_nf, hidden_nf),\n",
    "            act_fn,\n",
    "            nn.Linear(hidden_nf, output_nf))\n",
    "\n",
    "        layer = nn.Linear(hidden_nf, 1, bias=False)\n",
    "        # Initializes layer weights using xavier uniform initialization\n",
    "        torch.nn.init.xavier_uniform_(layer.weight, gain=0.001)\n",
    "\n",
    "        # Update coodinates\n",
    "        if not static_coord:\n",
    "            # coordinates mlp sequntial layers\n",
    "            coord_mlp = []\n",
    "            coord_mlp.append(nn.Linear(hidden_nf, hidden_nf))\n",
    "            coord_mlp.append(act_fn)\n",
    "            coord_mlp.append(layer)\n",
    "        \n",
    "        # attention mlp layer\n",
    "        if self.attention:\n",
    "            self.att_mlp = nn.Sequential(\n",
    "                nn.Linear(hidden_nf, 1),\n",
    "                nn.Sigmoid())\n",
    "            \n",
    "    def edge_model(self, source, target, radial, edge_feats):\n",
    "        # concatenation of edge features\n",
    "        if edge_feats is None:  # Unused.\n",
    "            out = torch.cat([source, target, radial], dim=1)\n",
    "        else:\n",
    "            # Dimension analysis:\n",
    "            # eg. source, target -> (num_edges, num_node_features)\n",
    "            # radial -> (num_edges, 1)\n",
    "            # edge_feats -> (num_edges, 3)\n",
    "            # out -> (num_edges, num_node_features*2 + 1 + 3)\n",
    "            out = torch.cat([source, target, radial, edge_feats], dim=1)\n",
    "        out = self.edge_mlp(out)\n",
    "        if self.attention:\n",
    "            att_val = self.att_mlp(out)\n",
    "            out = out * att_val\n",
    "        return out\n",
    "\n",
    "    def node_model(self, x, edge_index, edge_feats, node_attr = None):\n",
    "        # Dimension analysis:\n",
    "        # x -> (num_nodes, num_node_features)\n",
    "        # edge_index -> (2, num_edges)\n",
    "        # edge_feats -> (num_edges, num_edge_feats)\n",
    "\n",
    "        # unpacks source and target nodes from edge_index\n",
    "        row, col = edge_index\n",
    "        # unsorted_segment_sum sums up all edge features for each node\n",
    "        # agg dimension -> (num_nodes, num_edge_feats)\n",
    "        agg = unsorted_segment_sum(edge_feats, row, num_segments=x.size(0))\n",
    "        if node_attr is not None:\n",
    "            agg = torch.cat([x, agg, node_attr], dim=1)\n",
    "        else:\n",
    "            agg = torch.cat([x, agg], dim=1)\n",
    "        out = self.node_mlp(agg)\n",
    "        if self.residual:\n",
    "            out = x + out\n",
    "        return out, agg\n",
    "\n",
    "    def coord_model(self, coord, edge_index, coord_diff, edge_feat):\n",
    "        row, col = edge_index\n",
    "        trans = coord_diff * self.coord_mlp(edge_feat)\n",
    "        if self.coords_agg == 'sum':\n",
    "            agg = unsorted_segment_sum(trans, row, num_segments=coord.size(0))\n",
    "        elif self.coords_agg == 'mean':\n",
    "            agg = unsorted_segment_mean(trans, row, num_segments=coord.size(0))\n",
    "        else:\n",
    "            raise Exception('Wrong coords_agg parameter' % self.coords_agg)\n",
    "        coord += agg\n",
    "        return coord\n",
    "\n",
    "    def coord2radial(self, edge_index, coord):\n",
    "        # unpacks source and target nodes from edge_index\n",
    "        row, col = edge_index\n",
    "        # calculate coordinate difference between node\n",
    "        coord_diff = coord[row] - coord[col]\n",
    "        # calculate the radial distance for each pair of node\n",
    "        radial = torch.sum(coord_diff**2, 1).unsqueeze(1)\n",
    "        # normalization\n",
    "        if self.normalize:\n",
    "            norm = torch.sqrt(radial).detach() + self.epsilon\n",
    "            coord_diff = coord_diff / norm\n",
    "\n",
    "        return radial, coord_diff\n",
    "\n",
    "    def forward(self, h, edge_index, coord, add_edge_feats=None, node_attr=None):\n",
    "        # unpacks source and target nodes from edge_index\n",
    "        row, col = edge_index\n",
    "        # calculate radial distances for each pair of node\n",
    "        radial, coord_diff = self.coord2radial(edge_index, coord)\n",
    "        # Compute edge features\n",
    "        edges = self.edge_model(h[row], h[col], radial, add_edge_feats)\n",
    "\n",
    "        # Update coordinates\n",
    "        if not self.static_coord:\n",
    "            coord = self.coord_model(coord, edge_index, coord_diff, add_edge_feats)\n",
    "            \n",
    "        # Update node features\n",
    "        h, agg = self.node_model(h, edge_index, edges, node_attr)\n",
    "\n",
    "        return h, add_edge_feats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <i>Transformer Encoder</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Attention is All you Need\n",
    "    Reference: A. Vaswani et al., https://arxiv.org/abs/1706.03762\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, num_heads, num_ffn, dropout_r, act_fn_ecd):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.attention = MultiHeadAttention(d_model = d_model, num_heads = num_heads)\n",
    "        self.norm1 = LayerNormalization(parameters_shape = [d_model])\n",
    "        self.dropout = nn.Dropout(p = dropout_r)\n",
    "        self.ffn = MLP(d_model = d_model, num_ffn = num_ffn, act_fn = act_fn_ecd, dropout_r = dropout_r)\n",
    "        self.norm2 = LayerNormalization(parameters_shape = [d_model])\n",
    "        self.act_fn = act_fn_ecd\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Input value that's untouched and will be added to normalization layer\n",
    "        residual_x = x\n",
    "        # Calcualte self-attention matrix\n",
    "        x = self.attention(x)\n",
    "        # Dropout\n",
    "        x = self.dropout(x)\n",
    "        # Add residual_x to x and normalize\n",
    "        x = self.norm1(x + residual_x)\n",
    "        # Create another untouched residual input x\n",
    "        residual_x = x\n",
    "        # Pass to the feedforward neural network\n",
    "        x = self.ffn(x)\n",
    "        # Dropout\n",
    "        x = self.dropout(x)\n",
    "        # Add residual_x to the x and normalize again\n",
    "        x = self.norm2(x + residual_x)\n",
    "        # Output of encoder layer\n",
    "        return x\n",
    "\n",
    "# Multi-head attention class\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model                              # for example 512\n",
    "        self.num_heads = num_heads                          # for example for 8 heads\n",
    "        self.head_dim = d_model // num_heads                # head_dim will be 64\n",
    "        self.qkv_layer = nn.Linear(d_model, 3 * d_model)    # 512 x 1536\n",
    "        self.linear_layer = nn.Linear(d_model, d_model)     # 512 x 512\n",
    "\n",
    "    def forward(self, x):\n",
    "        if x.dim() == 2:\n",
    "            x = x.unsqueeze(0)  # Add a batch dimension at the front\n",
    "        batch_size, sequence_length, d_model = x.size()     # for example 30 x 200 x 512\n",
    "        #sequence_length, d_model = x.size()     # for example 30 x 200 x 512\n",
    "        qkv = self.qkv_layer(x) # 30 x 200 x 1536\n",
    "        qkv = qkv.reshape(batch_size, sequence_length, self.num_heads, 3 * self.head_dim) # 30 x 200 x 8 x 192\n",
    "        #qkv = qkv.reshape(sequence_length, self.num_heads, 3 * self.head_dim) # 30 x 200 x 8 x 192\n",
    "        qkv = qkv.permute(0, 2, 1, 3) # 30 x 8 x 200 x 192\n",
    "        q, k, v = qkv.chunk(3, dim = -1) # breakup using the last dimension, each are 30 x 8 x 200 x 64\n",
    "\n",
    "        values, attention = single_head_attention(q, k, v)\n",
    "        values = values.reshape(batch_size, sequence_length, self.num_heads * self.head_dim)\n",
    "        #values = values.reshape(sequence_length, self.num_heads * self.head_dim)\n",
    "        out = self.linear_layer(values)\n",
    "\n",
    "        return out\n",
    "        \n",
    "# Layer normalization\n",
    "class LayerNormalization(nn.Module):\n",
    "    def __init__(self, parameters_shape, eps = 1e-8):\n",
    "        super().__init__()\n",
    "        self.eps = eps # to take care of zero division\n",
    "        self.parameters_shape = parameters_shape\n",
    "        self.gamma = nn.Parameter(torch.ones(parameters_shape)) # learnable parameter \"std\" (512,)\n",
    "        self.beta = nn.Parameter(torch.zeros(parameters_shape)) # learnable parameter \"mean\" (512,)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        dims = [-(i+1) for i in range(len(self.parameters_shape))]\n",
    "        mean = inputs.mean(dim = dims, keepdim = True) # eg. for (30, 200, 512) inputs, mean -> (30, 200, 1)\n",
    "        var = ((inputs - mean)**2).mean(dim = dims, keepdim = True) # (30, 200, 1) \n",
    "        std = (var + self.eps).sqrt() # (30, 200, 1)\n",
    "        y = (inputs - mean) / std # Normalized output (30, 200, 512)\n",
    "        out = self.gamma*y + self.beta # Apply learnable parameters\n",
    "\n",
    "        return out\n",
    "\n",
    "# Feedforward MLP\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, d_model, num_ffn, act_fn, dropout_r = 0.1):\n",
    "        super(MLP, self).__init__()\n",
    "        self.linear1 = nn.Linear(d_model, num_ffn)\n",
    "        self.linear2 = nn.Linear(num_ffn, d_model)\n",
    "        self.act_fn = act_fn\n",
    "        self.dropout = nn.Dropout(p = dropout_r)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(self.act_fn(x))\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b><i>Final Model: Equivariant Graph Transformer<i></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt Text!](../images/architecture.png)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EGTF(nn.Module):\n",
    "    def __init__(self, # EGNN/EGCL parameters\n",
    "                 hidden_channels, num_edge_feats = 0, num_egcl = 4, \n",
    "                 act_fn = nn.SiLU(), residual = True, attention = True,\n",
    "                 normalize = False, max_atom_type = 100, \n",
    "                 cutoff = 5.0, max_num_neighbors = 32, static_coord = True,\n",
    "                 # Transformer-Encoder parameters\n",
    "                 d_model = 256, num_encoder = 2, num_heads = 8,\n",
    "                 num_ffn = 512, act_fn_ecd = nn.SiLU(), dropout_r = 0.1,\n",
    "                 # Energy Head parameter\n",
    "                 num_neurons = 512):\n",
    "\n",
    "        super(EGTF, self).__init__()\n",
    "        # self.hidden_channels = hidden_channels\n",
    "        self.n_layers = num_egcl\n",
    "        # self.max_atom_type = max_atom_type\n",
    "        self.cutoff = cutoff\n",
    "        self.max_num_neighbors = max_num_neighbors\n",
    "        # Create embeddings of dimension (hidden_channels, ) for each atom type\n",
    "        self.type_embedding = nn.Embedding(max_atom_type, hidden_channels)\n",
    "        \n",
    "        # EGC layers\n",
    "        for i in range(0, num_egcl):\n",
    "            self.add_module(\"gcl_%d\" % i, E_GCL(\n",
    "                input_nf = hidden_channels, \n",
    "                output_nf = hidden_channels, \n",
    "                hidden_nf = hidden_channels, \n",
    "                add_edge_feats = num_edge_feats,\n",
    "                act_fn = act_fn, residual = residual, \n",
    "                attention = attention, normalize = normalize,\n",
    "                static_coord = static_coord))\n",
    "\n",
    "        # Transformer-Encoder layers\n",
    "        self.encoder_layers = \\\n",
    "            nn.ModuleList([EncoderLayer(d_model, num_heads,\n",
    "                                        num_ffn, dropout_r, act_fn_ecd) \n",
    "                                        for _ in range(num_encoder)])\n",
    "\n",
    "        # Energy Head\n",
    "        self.energy_fc = nn.Sequential(\n",
    "            nn.Linear(num_neurons, num_neurons),\n",
    "            act_fn_ecd,\n",
    "            nn.Linear(num_neurons, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, z, pos, batch, edge_index=None, edge_feats=None):\n",
    "        h = self.type_embedding(z)\n",
    "        x = deepcopy(pos)\n",
    "\n",
    "        if edge_index is None:\n",
    "            # Calculates edge_index from graph structure based on cutoff radius\n",
    "            edge_index = radius_graph(\n",
    "                pos,\n",
    "                r=self.cutoff,\n",
    "                batch=batch,\n",
    "                loop=False,\n",
    "                max_num_neighbors=self.max_num_neighbors + 1,\n",
    "            )\n",
    "        # EGC layers\n",
    "        for i in range(0, self.n_layers):\n",
    "            h, _ = self._modules[\"gcl_%d\" % i](h, edge_index, x, add_edge_feats=edge_feats)\n",
    "        # Encoder layers\n",
    "        for layer in self.encoder_layers:\n",
    "            h = layer(h)\n",
    "        # Energy Head\n",
    "        h = h.squeeze(0)  # Assuming the batch dimension is at dim 0\n",
    "        h = scatter(h, batch, dim=0, reduce='add')\n",
    "        \n",
    "        out = self.energy_fc(h)\n",
    "\n",
    "        return out        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <b>Data Reading and Pre-Processing</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Workflow Explained:\n",
    "For complete pre-processing of data, \n",
    "1) `ANI1Wrapper` class will be created, which takes in batch size, num_workers, validation set size, test set size, data directory path, and seed as arguments. It will first utilize `anidataloader` function to store all atom coordinates, types and energies. Then it will split all data into training, validation, and test set using the specified values. \n",
    "\n",
    "    ```\n",
    "    self.train_species.extend([S] * len(train_idx))\n",
    "    self.train_energies.append(E[train_idx])\n",
    "    for i in train_idx:\n",
    "        self.train_positions.append(X[i])\n",
    "    ```\n",
    "    The atoms will be the same for each molecule, for example for water which is `['H','H','O']` and 3 conformations, we would create a list of `[['H''H','O'], ['H','H','O'], ['H','H','O']]` using `extend`.<br>`self.train_energies` will be a 1D list which holds the energies for each conformation.<br>`self.train_positions` will be a list of lists for example `X = [[pos1], [pos2], [pos3]]`.\n",
    "2) The splitted data would then be used to create `class ANI1(Dataset)`, which is a pre-processed Dataset class used for `torch_geometric.DataLoader`. When called, `class ANI1(Dataset)` will prepare one-hot encoded atom types, coordinates, and compensated energies (record energy - self interaction energy) into pytorch tensors batch-wise.\n",
    "3) Data loaders were created using `PyGDataLoader/torch_geometric.DataLoader` with parameters such as `num_workers`, `shuffle`, and `drop_last`. The meanings of these parameters were explained in inline documentation. `train_loader`, `valid_loader`, `test_loader` were returned, which are ready to be used by the Trainer class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import anidataloader\n",
    "from torch.utils.data import Dataset\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader as PyGDataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ANI1Wrapper(object):\n",
    "    def __init__(self, batch_size, num_workers, valid_size, test_size, data_dir, seed):\n",
    "        super(object, self).__init__()\n",
    "        \"\"\"\n",
    "        batch_size: training batch size\n",
    "        num_workers: Number of subprocesses to use for data loading, controls the parallelism\n",
    "        valid_size: validation set percentage\n",
    "        test_size: test set percentage\n",
    "        data_dir: file path for the Data directory\n",
    "        seed: random seed\n",
    "        \"\"\"\n",
    "        self.data_dir = data_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.valid_size = valid_size\n",
    "        self.test_size = test_size\n",
    "        self.seed = seed\n",
    "\n",
    "    def get_data_loaders(self):\n",
    "        random_state = np.random.RandomState(seed=self.seed)\n",
    "        # read all data with that ends wit hh5\n",
    "        hdf5files = [f for f in os.listdir(self.data_dir) if f.endswith('.h5')]\n",
    "\n",
    "        n_mol = 0 # to record the total number of molecules/conformations\n",
    "        self.train_species, self.valid_species, self.test_species = [], [], []\n",
    "        self.train_positions, self.valid_positions, self.test_positions = [], [], []\n",
    "        self.train_energies, self.valid_energies, self.test_energies = [], [], []\n",
    "        self.test_smiles = []\n",
    "        \n",
    "        for f in hdf5files:\n",
    "            print('reading:', f)\n",
    "            h5_loader = anidataloader(os.path.join(self.data_dir, f))\n",
    "            for data in h5_loader:\n",
    "                # Get coordinates, atom types, energies, smiles\n",
    "                X = data['coordinates']\n",
    "                S = data['species']\n",
    "                E = data['energies']\n",
    "                # get the total number of conformations\n",
    "                n_conf = E.shape[0]\n",
    "                # create a list of indices and randomly shuffle them\n",
    "                indices = list(range(n_conf))\n",
    "                random_state.shuffle(indices)\n",
    "                # calculate the split points for training, validation, and test data\n",
    "                split1 = int(np.floor(self.valid_size * n_conf))\n",
    "                split2 = int(np.floor(self.test_size * n_conf))\n",
    "                # split indices to 3\n",
    "                valid_idx, test_idx, train_idx = \\\n",
    "                    indices[:split1], indices[split1:split1+split2], indices[split1+split2:]\n",
    "\n",
    "                # Record atom types, training energies, and positions for the 3 datasets\n",
    "                self.train_species.extend([S] * len(train_idx))\n",
    "                self.train_energies.append(E[train_idx])\n",
    "                for i in train_idx:\n",
    "                    self.train_positions.append(X[i])\n",
    "                \n",
    "                self.valid_species.extend([S] * len(valid_idx))\n",
    "                self.valid_energies.append(E[valid_idx])\n",
    "                for i in valid_idx:\n",
    "                    self.valid_positions.append(X[i])\n",
    "                \n",
    "                self.test_species.extend([S] * len(test_idx))\n",
    "                self.test_energies.append(E[test_idx])\n",
    "                for i in test_idx:\n",
    "                    self.test_positions.append(X[i])\n",
    "\n",
    "                n_mol += 1\n",
    "            \n",
    "            h5_loader.cleanup()\n",
    "        \n",
    "        # Merge all lists of lists from all files into a single array\n",
    "        self.train_energies = np.concatenate(self.train_energies, axis=0)\n",
    "        self.valid_energies = np.concatenate(self.valid_energies, axis=0)\n",
    "        self.test_energies = np.concatenate(self.test_energies, axis=0)\n",
    "\n",
    "        print(\"# molecules:\", n_mol)\n",
    "        print(\"# train conformations:\", len(self.train_species))\n",
    "        print(\"# valid conformations:\", len(self.valid_species))\n",
    "        print(\"# test conformations:\", len(self.test_species))\n",
    "\n",
    "        train_dataset = ANI1(\n",
    "            self.data_dir, species=self.train_species,\n",
    "            positions=self.train_positions, energies=self.train_energies\n",
    "        )\n",
    "        valid_dataset = ANI1(\n",
    "            self.data_dir, species=self.valid_species,\n",
    "            positions=self.valid_positions, energies=self.valid_energies\n",
    "        )\n",
    "        test_dataset = ANI1(\n",
    "            self.data_dir, species=self.test_species, \n",
    "            positions=self.test_positions, energies=self.test_energies, \n",
    "            # smiles=self.test_smiles\n",
    "        )\n",
    "\n",
    "        \"\"\"\n",
    "        num_workers: specifies how many subprocesses to use for data loading. It controls the parallelism of data loading\n",
    "        shuffle: determines whether the data should be shuffled at every epoch, turned on for training and off for others\n",
    "        drop_last: controls whether the last batch should be dropped in case it is smaller than the specified batch_size\n",
    "        pin_memory: when set to True, enables the DataLoader to use pinned memory for faster data transfer to CUDA-enabled GPUs\n",
    "        persistent_workers: control whether the worker processes of the DataLoader should be kept alive across multiple iterations\n",
    "        \"\"\"\n",
    "        train_loader = PyGDataLoader(\n",
    "            train_dataset, batch_size=self.batch_size, num_workers=self.num_workers, \n",
    "            shuffle=True, drop_last=True, \n",
    "            pin_memory=True, persistent_workers=True\n",
    "        )\n",
    "        valid_loader = PyGDataLoader(\n",
    "            valid_dataset, batch_size=self.batch_size, num_workers=self.num_workers, \n",
    "            shuffle=False, drop_last=True, \n",
    "            pin_memory=True, persistent_workers=True\n",
    "        )\n",
    "        test_loader = PyGDataLoader(\n",
    "            test_dataset, batch_size=self.batch_size, num_workers=self.num_workers, \n",
    "            shuffle=False, drop_last=False\n",
    "        )\n",
    "\n",
    "        del self.train_species, self.valid_species, self.test_species\n",
    "        del self.train_positions, self.valid_positions, self.test_positions\n",
    "        del self.train_energies, self.valid_energies, self.test_energies\n",
    "\n",
    "        return train_loader, valid_loader, test_loader\n",
    "    \n",
    "\n",
    "ATOM_DICT = {('Br', 0): 1, ('C', 0): 3, ('Cl', 0): 7, ('F', 0): 9, ('H', 0): 10, ('I', 0): 12, ('N', 0): 17,\n",
    "            ('O', 0): 21, ('P', 0): 23, ('S', 0): 26}\n",
    "SELF_INTER_ENERGY = {\n",
    "    'H': -0.500607632585, \n",
    "    'C': -37.8302333826,\n",
    "    'N': -54.5680045287,\n",
    "    'O': -75.0362229210\n",
    "}\n",
    "\n",
    "class ANI1(Dataset):\n",
    "    def __init__(self, data_dir, species, positions, energies):\n",
    "        self.data_dir = data_dir\n",
    "        self.species = species\n",
    "        self.positions = positions\n",
    "        self.energies = energies\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # get the position, atoms, and energie for one conformation\n",
    "        # Index automatically handlled by PyGDataLoader\n",
    "        pos = self.positions[index]\n",
    "        atoms = self.species[index]\n",
    "        y = self.energies[index]\n",
    "\n",
    "        x = []\n",
    "        self_energy = 0.0\n",
    "\n",
    "        for atom in atoms:\n",
    "            # calculate cumulative self interaction energy\n",
    "            x.append(ATOM_DICT[(atom, 0)])\n",
    "            self_energy += SELF_INTER_ENERGY.get(atom, 0)\n",
    "\n",
    "        x = torch.tensor(x, dtype=torch.long)\n",
    "        pos = torch.tensor(pos, dtype=torch.float)\n",
    "        # Hartree to kcal/mol\n",
    "        y = torch.tensor(y, dtype=torch.float).view(1,-1) * 627.5\n",
    "        # Hartree to kcal/mol\n",
    "        self_energy = torch.tensor(self_energy, dtype=torch.float).view(1,-1) * 627.5\n",
    "        data = Data(x=x, pos=pos, y=y-self_energy, self_energy=self_energy)\n",
    "\n",
    "        return data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.positions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <b>Trainer Class</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Workflow Explained:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizer class for normalizing and denormalizing energy values\n",
    "class Normalizer(object):\n",
    "    \"\"\"Class for normalization and de-normalization of tensors. \"\"\"\n",
    "\n",
    "    def __init__(self, tensor):\n",
    "        \"\"\"tensor is taken as a sample to calculate the mean and std\"\"\"\n",
    "        self.mean = torch.mean(tensor)\n",
    "        self.std = torch.std(tensor)\n",
    "\n",
    "    def norm(self, tensor):\n",
    "        return (tensor - self.mean) / self.std\n",
    "\n",
    "    def denorm(self, normed_tensor):\n",
    "        return normed_tensor * self.std + self.mean\n",
    "\n",
    "    def state_dict(self):\n",
    "        return {'mean': self.mean,\n",
    "                'std': self.std}\n",
    "\n",
    "    def load_state_dict(self, state_dict):\n",
    "        self.mean = state_dict['mean']\n",
    "        self.std = state_dict['std']\n",
    "\n",
    "# Trainer class\n",
    "class Trainer(object):\n",
    "\n",
    "    def __init__(self, model, config):\n",
    "        # Get config and device\n",
    "        self.model = model\n",
    "        self.config = config\n",
    "        self.device = self._get_device()\n",
    "        # Data processing using ANIWrapper\n",
    "        self.dataset = ANI1Wrapper(**self.config['dataset_dict'])\n",
    "        # Prefix for log directory names\n",
    "        self.prefix = 'ani1'\n",
    "        self.model_prefix = 'egtf'\n",
    "        dir_name = '_'.join([datetime.now().strftime('%b%d_%H-%M-%S'), self.prefix, self.model_prefix])\n",
    "        self.log_dir = os.path.join('Runs', dir_name)\n",
    "        self.writer = SummaryWriter(log_dir=self.log_dir)\n",
    "\n",
    "    # Get device function\n",
    "    def _get_device(self):\n",
    "        if torch.cuda.is_available() and self.config['gpu'] != 'cpu':\n",
    "            device = self.config['gpu']\n",
    "        else:\n",
    "            device = 'cpu'\n",
    "        print(\"Running on:\", device)\n",
    "\n",
    "        return device\n",
    "\n",
    "    # config saving\n",
    "    @staticmethod\n",
    "    def _save_config_file(ckpt_dir):\n",
    "        if not os.path.exists(ckpt_dir):\n",
    "            os.makedirs(ckpt_dir)\n",
    "            shutil.copy('./config.yaml', os.path.join(ckpt_dir, 'config.yaml'))\n",
    "\n",
    "    # loss function\n",
    "    def loss_fn(self, model, data):\n",
    "        data = data.to(self.device)\n",
    "        pred_e = model(data.x, data.pos, data.batch)\n",
    "        loss = F.mse_loss(\n",
    "            pred_e, self.normalizer.norm(data.y), reduction='mean'\n",
    "        )\n",
    "        return pred_e, loss\n",
    "\n",
    "    def train(self):\n",
    "        # load training, validation, test loader\n",
    "        train_loader, valid_loader, test_loader = self.dataset.get_data_loaders()\n",
    "\n",
    "        energy_labels = []\n",
    "        for data in train_loader:\n",
    "            energy_labels.append(data.y)\n",
    "        labels = torch.cat(energy_labels)\n",
    "        # normalize energy values\n",
    "        self.normalizer = Normalizer(labels)\n",
    "        gc.collect() # free memory\n",
    "\n",
    "        model = self.model.to(self.device)\n",
    "\n",
    "        # Read in training learning rate parameters\n",
    "        if type(self.config['lr']) == str: self.config['lr'] = eval(self.config['lr']) \n",
    "        if type(self.config['min_lr']) == str: self.config['min_lr'] = eval(self.config['min_lr'])\n",
    "        if type(self.config['weight_decay']) == str: self.config['weight_decay'] = eval(self.config['weight_decay']) \n",
    "        \n",
    "        optimizer = AdamW(\n",
    "            model.parameters(), self.config['lr'],\n",
    "            weight_decay=self.config['weight_decay'],\n",
    "        )\n",
    "\n",
    "        ckpt_dir = os.path.join(self.writer.log_dir, 'checkpoints')\n",
    "        self._save_config_file(ckpt_dir)\n",
    "\n",
    "        n_iter = 0\n",
    "        valid_n_iter = 0\n",
    "        best_valid_loss = np.inf\n",
    "\n",
    "        from utils import adjust_learning_rate  # This is a function that adjusts learning rate according to\n",
    "                                                # specified lr, min_lr, epochs, warmup_epochs, patience_epochs\n",
    "\n",
    "        for epoch_counter in range(self.config['epochs']):\n",
    "            for bn, data in enumerate(train_loader):\n",
    "                # adjust learning rate accordingly                \n",
    "                adjust_learning_rate(optimizer, epoch_counter + bn / len(train_loader), self.config)\n",
    "                # use custom loss function because of normalization\n",
    "                __, loss = self.loss_fn(model, data)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                # record training loss and current learning rate\n",
    "                if n_iter % self.config['log_every_n_steps'] == 0:\n",
    "                    self.writer.add_scalar('loss', loss.item(), global_step=n_iter)\n",
    "                    self.writer.add_scalar('lr', optimizer.param_groups[0]['lr'], global_step=n_iter)\n",
    "                    print(f\"Training at Epoch: {epoch_counter+1}, Batch #: {bn+1}, RMSE: {loss.item()}\")\n",
    "                    torch.cuda.empty_cache()\n",
    "\n",
    "                n_iter += 1\n",
    "            \n",
    "            gc.collect() # free memory\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            # validate the model \n",
    "            valid_rmse = self._validate(model, valid_loader)\n",
    "            self.writer.add_scalar('valid_rmse', valid_rmse, global_step=valid_n_iter)\n",
    "            print(f\"Validation at Epoch: {epoch_counter+1}, RMSE: {valid_rmse}\")\n",
    "\n",
    "            if valid_rmse < best_valid_loss:\n",
    "                best_valid_loss = valid_rmse\n",
    "                torch.save(model.state_dict(), os.path.join(ckpt_dir, 'best_model.pth'))\n",
    "\n",
    "            valid_n_iter += 1\n",
    "        \n",
    "        start_time = time.time()\n",
    "        self._test(model, test_loader)\n",
    "        print('Test duration:', time.time() - start_time)\n",
    "\n",
    "    # Validation set function\n",
    "    def _validate(self, model, valid_loader):\n",
    "        predictions, labels = [], []\n",
    "        model.eval()\n",
    "\n",
    "        for bn, data in enumerate(valid_loader):\n",
    "            pred_e, loss = self.loss_fn(model, data)\n",
    "            pred_e = self.normalizer.denorm(pred_e)\n",
    "\n",
    "            y = data.y\n",
    "\n",
    "            if self.device == 'cpu':\n",
    "                predictions.extend(pred_e.flatten().detach().numpy())\n",
    "                labels.extend(y.flatten().numpy())\n",
    "            else:\n",
    "                predictions.extend(pred_e.flatten().cpu().detach().numpy())\n",
    "                labels.extend(y.cpu().flatten().numpy())\n",
    "            \n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        gc.collect() # free memory\n",
    "\n",
    "        model.train()\n",
    "        return mean_squared_error(labels, predictions, squared=False)\n",
    "    \n",
    "    # Test set function\n",
    "    def _test(self, model, test_loader):\n",
    "        # Load the best validation model\n",
    "        model_path = os.path.join(self.log_dir, 'checkpoints', 'best_model.pth')\n",
    "        state_dict = torch.load(model_path, map_location=self.device)\n",
    "        model.load_state_dict(state_dict)\n",
    "        print(f\"Loaded {model_path} successfully.\")\n",
    "        \n",
    "        predictions, labels = [], []\n",
    "        model.eval()\n",
    "\n",
    "        for bn, data in enumerate(test_loader):                \n",
    "            pred_e, _ = self.loss_fn(model, data)\n",
    "            pred_e = self.normalizer.denorm(pred_e)\n",
    "\n",
    "            label = data.y\n",
    "\n",
    "            # add the self interaction energy back\n",
    "            pred_e += data.self_energy\n",
    "            label += data.self_energy\n",
    "\n",
    "            if self.device == 'cpu':\n",
    "                predictions.extend(pred_e.flatten().detach().numpy())\n",
    "                labels.extend(label.flatten().numpy())\n",
    "            else:\n",
    "                predictions.extend(pred_e.flatten().cpu().detach().numpy())\n",
    "                labels.extend(label.cpu().flatten().numpy())\n",
    "        \n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        gc.collect() # free memory\n",
    "\n",
    "        rmse = mean_squared_error(labels, predictions, squared=False)\n",
    "        mae = mean_absolute_error(labels, predictions)\n",
    "        print(f\"The test RMSE and MAE are {rmse}, {mae}\")\n",
    "\n",
    "        # Write the \n",
    "        with open(os.path.join(self.log_dir, 'results.csv'), mode='w', newline='') as csv_file:\n",
    "            csv_writer = csv.writer(csv_file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "            for i in range(len(labels)):\n",
    "                csv_writer.writerow([predictions[i], labels[i]])\n",
    "            csv_writer.writerow([rmse, mae])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <b>Model Training/Prediction</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(model_dict):\n",
    "    model = EGTF(\n",
    "        # EGNN/EGCL parameters\n",
    "        hidden_channels = model_dict[\"hidden_channels\"],\n",
    "        num_edge_feats = model_dict[\"num_edge_feats\"],\n",
    "        num_egcl = model_dict[\"num_egcl\"],\n",
    "        act_fn = model_dict[\"act_fn\"],\n",
    "        residual = model_dict[\"residual\"],\n",
    "        attention = model_dict[\"attention\"],\n",
    "        normalize = model_dict[\"normalize\"],\n",
    "        max_atom_type = model_dict[\"max_atom_type\"],\n",
    "        cutoff = model_dict[\"cutoff\"],\n",
    "        max_num_neighbors = model_dict[\"max_num_neighbors\"],\n",
    "        static_coord = model_dict[\"static_coord\"],\n",
    "        # Encoder Parameters\n",
    "        d_model = model_dict[\"d_model\"],\n",
    "        num_encoder = model_dict[\"num_encoder\"],\n",
    "        num_heads = model_dict[\"num_heads\"],\n",
    "        num_ffn = model_dict[\"num_ffn\"],\n",
    "        act_fn_ecd = nn.SiLU(),\n",
    "        dropout_r = model_dict[\"dropout_r\"],\n",
    "        # Energy head parameter\n",
    "        num_neurons = model_dict[\"num_neuron\"]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "def train_and_eval():\n",
    "    config = yaml.load(open(\"config.yaml\", \"r\"), Loader=yaml.FullLoader)\n",
    "    # Get correct activation function\n",
    "    act_fn_dict = {\"SiLU\": nn.SiLU(), \"ReLU\": nn.ReLU}\n",
    "    config[\"model_dict\"][\"act_fn\"] = act_fn_dict[config[\"model_dict\"][\"act_fn\"]]\n",
    "    config[\"model_dict\"][\"act_fn_ecd\"] = act_fn_dict[config[\"model_dict\"][\"act_fn_ecd\"]]\n",
    "\n",
    "    print(config)\n",
    "    model = create_model(config[\"model_dict\"])\n",
    "    \n",
    "    total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"The total number of parameters is {total_params}\")\n",
    "    \n",
    "    if config[\"load_model\"]:\n",
    "        # Load pre-trained model\n",
    "        pretrained_model = torch.load(config[\"load_model\"], map_location=torch.device('cpu'))\n",
    "        model.load_state_dict(pretrained_model, strict=False);\n",
    "    \n",
    "    trainer = Trainer(model, config)\n",
    "    trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of parameters is 863746\n"
     ]
    }
   ],
   "source": [
    "config_dict = {\n",
    "    \"gpu\": \"cuda:0\", \"lr\": 2e-4,\n",
    "    \"min_lr\": 1e-7, \"weight_decay\": 0.0,\n",
    "    \"epochs\": 1, \"warmup_epochs\": 0.7,\n",
    "    \"patience_epochs\": 0.3,\n",
    "    \"log_every_n_steps\": 50,\n",
    "    # \"Load_model\": None\n",
    "    \"load_model\": \"pretrained_egnn.pth\",\n",
    "    \n",
    "    \"model_dict\": {\n",
    "        \"hidden_channels\": 256,    # Number of hidden_channels\n",
    "        \"num_edge_feats\": 0,       # Number of additional edge features\n",
    "        \"num_egcl\": 1,             # Number of EGCL layers\n",
    "        \"act_fn\": \"SiLU\",\n",
    "        \"residual\": True,          # Residual calculation\n",
    "        \"attention\": True,         # Graph Attention mechanism\n",
    "        \"normalize\": True,         # Interatomic distance normalization\n",
    "        \"cutoff\": 5,             # Interatomic distance curoff\n",
    "        \"max_atom_type\": 28,       # Max atom types\n",
    "        \"max_num_neighbors\": 32,   # Max number of neighborgoods\n",
    "        \"static_coord\": True,      # Specify whether to update coord or not\n",
    "\n",
    "        # Transformer-Encoder part of the model\n",
    "        \"d_model\": 256,            # Embeddings for each token\n",
    "        \"num_encoder\": 1,          # Number of encoder units\n",
    "        \"num_heads\": 4,            # Number of self-attention heads\n",
    "        \"num_ffn\": 256,            # Number of neurons in the feedforward MLP\n",
    "        \"act_fn_ecd\": \"ReLU\",   # Activation function for encoder MLP\n",
    "        \"dropout_r\": 0.2,          # Dropout rate\n",
    "\n",
    "        # Energy Head\n",
    "        \"num_neuron\": 256         # NUmber of neurons for the final energy head\n",
    "    },\n",
    "\n",
    "    \"dataset_dict\": {\n",
    "        \"batch_size\": 128, \"num_workers\": 2,\n",
    "        \"valid_size\": 0.5, \"test_size\": 0.3,\n",
    "        \"data_dir\": '../Data', \"seed\": 666,\n",
    "    }\n",
    "}\n",
    "# Write parameters to yaml\n",
    "yaml_str = yaml.dump(config_dict)\n",
    "with open('config.yaml', 'w') as file:\n",
    "    file.write(yaml_str)\n",
    "config = yaml.load(open(\"config.yaml\", \"r\"), Loader=yaml.FullLoader)\n",
    "\n",
    "# Get the correct activation functions\n",
    "act_fn_dict = {\"SiLU\": nn.SiLU(), \"ReLU\": nn.ReLU}\n",
    "config[\"model_dict\"][\"act_fn\"] = act_fn_dict[config[\"model_dict\"][\"act_fn\"]]\n",
    "config[\"model_dict\"][\"act_fn_ecd\"] = act_fn_dict[config[\"model_dict\"][\"act_fn_ecd\"]]\n",
    "\n",
    "# Create model and print the total number of learnable parameters\n",
    "model = create_model(config[\"model_dict\"])\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"The total number of parameters is {total_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Load_model': 'pretrained_egnn.pth', 'dataset_dict': {'batch_size': 128, 'data_dir': '../Data', 'num_workers': 2, 'seed': 666, 'test_size': 0.3, 'valid_size': 0.5}, 'epochs': 1, 'gpu': 'cuda:0', 'load_model': None, 'log_every_n_steps': 10, 'lr': 0.0002, 'min_lr': 1e-07, 'model_dict': {'act_fn': SiLU(), 'act_fn_ecd': <class 'torch.nn.modules.activation.ReLU'>, 'attention': True, 'cutoff': 5, 'd_model': 256, 'dropout_r': 0.2, 'hidden_channels': 256, 'max_atom_type': 28, 'max_num_neighbors': 32, 'normalize': True, 'num_edge_feats': 0, 'num_egcl': 2, 'num_encoder': 1, 'num_ffn': 256, 'num_heads': 4, 'num_neuron': 256, 'residual': True, 'static_coord': True}, 'patience_epochs': 0.3, 'warmup_epochs': 0.7, 'weight_decay': 0.0}\n",
      "The total number of parameters is 1258499\n",
      "Running on: cpu\n",
      "reading: ani1_gdb10_ts.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# molecules: 138\n",
      "# train conformations: 2208\n",
      "# valid conformations: 5105\n",
      "# test conformations: 3035\n",
      "Training at Epoch: 1, Batch #: 1, RMSE: 15.425966262817383\n",
      "Training at Epoch: 1, Batch #: 11, RMSE: 3.9385733604431152\n",
      "Validation at Epoch: 1, RMSE: 797.3909912109375\n",
      "Loaded {model_path} successfully.\n",
      "The test RMSE and MAE are 793.5399169921875, 712.322998046875\n",
      "Test duration: 14.912760019302368\n"
     ]
    }
   ],
   "source": [
    "train_and_eval()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_potential",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
